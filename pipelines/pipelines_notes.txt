Notes:

cat_lst=[]
num_lst=[]

# create a pipeline for both numeric and categorical features

numeric_transform = Pipeline([('scaling', StandardScaler())
                              ])

categorical_transform = Pipeline([('one-hot-encode', OneHotEncoder()),
                                  ('to_dense', ToDenseTransformer())
                                  ])


#combining num and cat pipelines in a pre-process pipeline
preprocess = ColumnTransformer([
    ('numerical_preprocessing', numeric_transform, num_lst),
    ('categorical_preprocessing', categorical_transform, cat_lst),
])


# creating paramaters and base models
test_params=[{'pca__n_components':[4,5,6],
    'base_model':[GradientBoostingClassifier()],
    'base_model__max_depth':[3,5,8,10,14],
    'base_model__n_estimators':[15,25,50,100,150]
    },
    {
    'pca__n_components':[2,3,4,5,6], 
    'base_model': [MultinomialNB()],
    #'base_model': (0.25, 0.5, 0.75, 1.0)
    }
    ]

# creating main pipeline
base_model=RandomForestClassifier() # main pipeline takes the random forrest classifier as an argument but switches it out with the base_models found in the param grid

pipeline = Pipeline([
    ('preprocess', preprocess),
    ('pca', PCA(n_components=3)),
    ('base_model', base_model)
])

# implementing
pipeline.fit(X_train,y_train)
grid = GridSearchCV(pipeline, param_grid=test_params,  cv=5)
grid.fit(X_train, y_train)


# testing fit
best_model = grid.best_estimator_
best_hyperparams = grid.best_params_
best_acc = grid.score(X_test, y_test)

print(best_acc)
best_hyperparams
